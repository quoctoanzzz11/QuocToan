{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import when\n",
    "import pyspark.sql.functions as sf\n",
    "import os \n",
    "import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\",\"2g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datevalue(value):\n",
    "\tdate_value = datetime.datetime.strptime(value,\"%Y%m%d\").date()\n",
    "\treturn date_value\n",
    "\n",
    "def date_range(start_date,end_date):\n",
    "\tdate_list = []\n",
    "\tcurrent_date = start_date\n",
    "\twhile current_date <= end_date:\n",
    "\t\tdate_list.append(current_date.strftime(\"%Y%m%d\"))\n",
    "\t\tcurrent_date += datetime.timedelta(days=1)\n",
    "\treturn date_list\n",
    "\n",
    "def generate_date_range(from_date,to_date):\n",
    "\tfrom_date = convert_to_datevalue(from_date)\n",
    "\tto_date = convert_to_datevalue(to_date)\n",
    "\tdate_list = date_range(from_date,to_date)\n",
    "\treturn date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_file(df):\n",
    "    df = df.withColumn(\"Type\",\n",
    "       when((col(\"AppName\") == 'CHANNEL') |  (col(\"AppName\") =='KPLUS'), \"TV\")\n",
    "      .when((col(\"AppName\") == 'VOD') | (col(\"AppName\") =='FIMS') , \"Movie\")\n",
    "      .when((col(\"AppName\") == 'CHILD'), \"Child\")\n",
    "      .when((col(\"AppName\") == 'RELAX'), \"Relax\")\n",
    "      .when((col(\"AppName\") == 'SPORT'), \"Sport\")\n",
    "      .otherwise(\"Error\"))\n",
    "    df = df.drop(df.AppName)\n",
    "    df = df.groupBy('Contract').pivot('Type').sum('TotalDuration')\n",
    "    df = df.fillna(0)\n",
    "    df = df.withColumnRenamed('Child','ChildDuration')\n",
    "    ds = df.withColumnRenamed('Movie','MovieDuration')\n",
    "    df = df.withColumnRenamed('Relax','RelaxDuration')\n",
    "    df = df.withColumnRenamed('Sport','SportDuration')\n",
    "    df = df.withColumnRenamed('TV','TVDuration')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_task(from_date,to_date):\n",
    "    path = \"E:\\\\log_content\\\\\"\n",
    "    dinh_dang = '.json'\n",
    "    file_name1 = spark.read.json(path+from_date+dinh_dang)\n",
    "    file_name1 = file_name1.select('_source.Contract','_source.AppName','_source.TotalDuration')\n",
    "    \n",
    "    list_file = os.listdir(path)\n",
    "    list_file_1 = generate_date_range(from_date,to_date)   \n",
    "\n",
    "    for i in list_file_1[1:]:\n",
    "        file_name2 = spark.read.json(path+i+dinh_dang)\n",
    "        file_name2 = file_name2.select('_source.Contract','_source.AppName','_source.TotalDuration')\n",
    "        \n",
    "        file_name1 = file_name1.union(file_name2)\n",
    "        file_name1 = file_name1.cache()\n",
    "    \n",
    "    final = etl_file(file_name1)\n",
    "    \n",
    "    final.write.csv('E:\\\\Output_logcontent_method\\\\clean_data',header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_task('20220402','20220405')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----+-------------+-------------+----------+\n",
      "| Contract|ChildDuration|Movie|RelaxDuration|SportDuration|TVDuration|\n",
      "+---------+-------------+-----+-------------+-------------+----------+\n",
      "|SGH348665|            0|    0|            0|            0|     20930|\n",
      "|SGH731784|            0|12981|            0|            0|     47541|\n",
      "|THFD18721|            0|    0|            0|            0|     94717|\n",
      "|THFD14003|            0|    0|            0|            0|    259200|\n",
      "|SGH395523|            0|    0|            0|            0|    269420|\n",
      "|HUFD59829|            0|    0|            0|            0|     46865|\n",
      "|HPFD86112|            0|    0|            0|            0|     14685|\n",
      "|SGH908987|          152|    0|            0|            0|    143830|\n",
      "|SGH299258|            0| 6600|            0|            0|     38093|\n",
      "|CTFD34334|            0|    0|            0|            0|    140055|\n",
      "|NAD030899|            0| 2947|            0|            0|      6041|\n",
      "|HUFD42807|            0|    0|            0|           36|    128716|\n",
      "|BTFD27637|            0|    0|            0|            0|     95011|\n",
      "|LAFD23456|            0|    0|            0|            0|    209693|\n",
      "|HNH005529|            0|21761|            0|            0|     10414|\n",
      "|SGH385767|            0|    0|            0|            0|     18846|\n",
      "|BLFD07827|          139|    0|            0|            0|     64350|\n",
      "|VTFD71145|            0|    0|            0|            0|    324770|\n",
      "|SGH546066|            0|    0|            0|            0|     67957|\n",
      "|VLFD11554|            0|    0|            0|            0|     86400|\n",
      "+---------+-------------+-----+-------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final = spark.read.csv(\"E:\\\\Output_logcontent_method\\\\clean_data\",header=True)\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
